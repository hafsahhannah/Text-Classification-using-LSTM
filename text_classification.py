# -*- coding: utf-8 -*-
"""Text Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TmzF4FVPnUcrLL8PZ03yh0IZlr1uZBl0
"""

import pandas as pd
from tensorflow.keras.layers import SimpleRNN,Dense,LSTM,Dropout,Embedding
from tensorflow.keras import Sequential, Input
from sklearn.metrics._plot.confusion_matrix import ConfusionMatrixDisplay
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import re
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from typing import Text
import textwrap
import numpy as np 
from keras_preprocessing.sequence import pad_sequences
from sklearn.preprocessing import OneHotEncoder
import pickle
import json
#rom modules import text_cleaning,lstm_model_creation

# 1)Data loading

path = "True.csv"
df = pd.read_csv(path)
print(df)

# Step 2 Data inspection

df.info()

df.describe().T

print(df.isna().sum())

# 3)Data cleaning


def text_cleaning(text):

  text = re.sub('bit.ly/\d\w{1,10}','',text)
  text = re.sub('@[\s]+','',text)
  text = re.sub('\[.*?EST\]','',text)
  text = re.sub('[^a-zA-Z]',' ',text).lower()

  return text

'''
  for index, data in enumerate (df['text']):
    df['text'][index] = re.sub('bit.ly/\d\w{1,10}','',data)
    df['text'][index] = re.sub('@[\s]+','',data)
    df['text'][index] = re.sub('\[.*?EST\]','',data)
    df['text'][index] = re.sub('[^a-zA-Z]',' ',df['text'][index]).lower()
'''

for index, temp in enumerate(df['text']):
  df['text'][index] = text_cleaning(temp)

temp = df['text'][14]
print (temp)

# Remove duplicate

df = df.drop_duplicates()
df.duplicated().sum()

# 4)Feature selection

text = df['text'].values
subject = df['subject'].values

# 5)Data preprocessing

# Load tokenizer

num_words = 5000

oov_token = '<OOV>'  # out of vocab

# from sklearn.preprocessing import MinMaxScaler
# mms = MinMaxScaler() # instantiate

tokenizer = Tokenizer(num_words=num_words, oov_token=oov_token)  # instantiate

tokenizer.fit_on_texts(text)
word_index = tokenizer.word_index
print(dict(list(word_index.items())[0:10]))

text = tokenizer.texts_to_sequences(text)

padded_text = pad_sequences(
    text, maxlen=200, padding='post', truncating='pre')

# One hot encoder

ohe = OneHotEncoder(sparse=False) # to change to list
subject = ohe.fit_transform(subject[::,None])

padded_text = np.expand_dims(padded_text,axis=-1)

X_train,X_test,y_train,y_test = train_test_split(padded_text,subject,test_size=0.2,random_state=123)

# Model development

#embedding_layer = 64
#num_words = 5000

def lstm_model_creation(num_words,embedding_layer=64,dropout=0.3,num_neurons=64):

  model = Sequential()
  model.add(Embedding(num_words,embedding_layer))
  model.add(LSTM(embedding_layer,return_sequences=True))
  model.add(Dropout(dropout))
  model.add(LSTM(num_neurons))
  model.add(Dropout(dropout))
  model.add(Dense(2,activation='softmax')) #cuz it return probability 
  model.summary()

  model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['acc'])

  return model

model = lstm_model_creation(num_words,subject.shape[1],dropout=0.4)

hist = model.fit(X_train,y_train,validation_data=(X_test,y_test),batch_size=64, epochs=5,callbacks=[])

# Model analysis

plt.figure()
plt.plot(hist.history['acc'])
plt.plot(hist.history['val_acc'])
plt.legend(['training','validation'])
plt.show()

y_predicted = model.predict(X_test)

# Classification report

y_predicted = np.argmax(y_predicted,axis=1)
y_test = np.argmax(y_test,axis=1)

print(classification_report(y_test,y_predicted))
print(confusion_matrix(y_test,y_predicted))

print(classification_report(y_test,y_predicted))
cm = confusion_matrix(y_test,y_predicted)

disp = ConfusionMatrixDisplay(cm)
disp.plot()

#model saving
model.save('model.h5')

#save ohe
with open('ohe.pkl','wb') as f:
  pickle.dump(ohe,f)

#tokenizer
with open('tokenizer.json','w') as f:
  json.dump(tokenizer.to_json(),f)